# Voice2Txt: Голосовой ввод с AI-коррекцией

Это утилита для быстрого голосового ввода, которая позволяет надиктовывать текст, удерживая горячую клавишу. Распознанный текст автоматически корректируется с помощью GPT и вставляется в активное окно.

<img width="1229" height="253" alt="image" src="https://github.com/user-attachments/assets/a339fcc9-9d92-4974-89e7-5df97a971e31" />


## Основные возможности (Features)

*   Запись голоса по удержанию горячей клавиши (`F9`).
*   Быстрое распознавание речи с помощью OpenAI Whisper.
*   Автоматическая коррекция, перевод и стилизация текста через GPT.
*   Гибкая настройка режимов обработки через файл `prompts.json`.
*   Удобный "паллет" настроек, вызываемый по `F10`, с полной поддержкой клавиатуры.
*   Звуковые уведомления о начале и конце записи.

## Установка и запуск

### Шаг 1: Клонирование репозитория
```bash
git clone https://github.com/your-username/voice2txt.git
cd voice2txt
```

### Шаг 2: Установка зависимостей
Рекомендуется использовать виртуальное окружение для изоляции зависимостей проекта.
```bash
python -m venv venv
source venv/bin/activate  # Для Windows: venv\Scripts\activate
```
Установите все необходимые библиотеки:
```bash
pip install -r requirements.txt
```

### Шаг 3: Настройка FFmpeg (Критически важно!)
Для работы с аудио библиотека Whisper требует наличия в системе `FFmpeg`.

*   **Windows:** Скачайте `ffmpeg` с [официального сайта](https://ffmpeg.org/download.html), распакуйте архив и добавьте путь к папке `bin` в системную переменную `PATH`.
*   **macOS (через Homebrew):**
    ```bash
    brew install ffmpeg
    ```
*   **Linux (Debian/Ubuntu):**
    ```bash
    sudo apt-get update && sudo apt-get install ffmpeg
    ```

### Шаг 4: Настройка API-ключей
Для работы GPT-коррекции необходимо указать ваши ключи OpenAI (или совместимого) API.

1.  Создайте файл с именем `.env` в корневой папке проекта.
2.  Добавьте в него следующие строки, заменив значения на ваши:

```dotenv
# Ваш ключ от OpenAI
OPENAI_API_KEY="sk-..."

# URL для доступа к API. Если вы используете прокси или альтернативный сервис, укажите его здесь.
# Например, для работы с локальной моделью через LM Studio это может быть "http://localhost:1234/v1"
OPENAI_API_BASE="https://api.openai.com/v1"
```

### Шаг 5: Запуск
После выполнения всех шагов запустите скрипт:
```bash
python voice2txt.py
```

## Использование

*   **`F9`:** Нажмите и удерживайте для записи голоса. Отпустите для распознавания и вставки текста.
*   **`F10`:** Нажмите для вызова паллета настроек. Используйте стрелки `Up`/`Down` для навигации, `Enter` для выбора режима и `Escape` для закрытия.

## Конфигурация и кастомизация

### Файл `prompts.json`
Это сердце вашего приложения. В этом файле вы можете настраивать режимы обработки текста.

*   **Ключ** (`"Коррекция"`, `"Перевод..."`) — это название режима, которое вы увидите в паллете настроек.
*   **Значение** — это системный промпт (инструкция) для GPT-модели.

Вы можете легко **добавлять, удалять и редактировать** режимы. Например, чтобы добавить режим для создания краткого содержания, просто допишите в файл:
```json
{
  "Коррекция": "...",
  "Перевод на разговорный Английский": "...",
  "Стилистика: Деловой стиль": "...",
  "Стилистика: Дружелюбный тон": "...",
  "Сделать саммари": "Ты — AI-ассистент, который делает краткое содержание текста. Проанализируй следующий текст и верни его основную мысль в одном-двух предложениях."
}
```

### Настройки в коде
Некоторые базовые параметры, такие как горячие клавиши (`HOTKEY`, `SETTINGS_HOTKEY`) и размер модели Whisper (`MODEL_SIZE`), на данный момент настраиваются прямо в коде файла `voice2txt.py`.

## Технические детали и решение проблем

### Устранение неполадок и архитектурные решения

*   **Производительность (CPU vs GPU):**
    Распознавание речи — ресурсоемкая задача. Скрипт автоматически определяет наличие CUDA-совместимой видеокарты NVIDIA и использует ее для ускорения вычислений. При запуске в логах будет указано, какое устройство используется (`CPU` или `CUDA`). Работа на CPU может быть значительно медленнее.

*   **"Прогрев" модели Whisper:**
    Первое распознавание речи после запуска скрипта может занять до 30 секунд. **Это нормально** — модель в этот момент "прогревается", подгружая все необходимые компоненты в память. Все последующие распознавания будут быстрыми и займут всего несколько секунд.

*   **Стабильность горячих клавиш (`pynput`):**
    Для перехвата нажатий клавиш используется библиотека `pynput`, так как она показала большую стабильность по сравнению с аналогами и решает проблему системных сбоев (`0xC0000096`) на некоторых конфигурациях Windows.

*   **Проблемы с GUI (Фокус окна):**
    Если окно настроек (`F10`) не получает фокус клавиатуры (т.е. не реагирует на стрелки), мы используем специальный трюк (`iconify`/`deiconify`), чтобы принудительно заставить операционную систему передать ему управление. Это обеспечивает надежную работу на разных оконных менеджерах.

## Зависимости
Основные зависимости проекта перечислены в файле `requirements.txt` и включают:
*   `openai-whisper`
*   `torch`
*   `pynput`
*   `customtkinter`
*   `openai`
*   `sounddevice`
*   и другие.

